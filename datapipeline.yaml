AWSTemplateFormatVersion: '2010-09-09'
Metadata: 
  License: Apache-2.0
Description: 'AWS CloudFormation Sample Template to create DataPipeline'
Resources:
 DynamoDBOutputS3InputHive: 
  Type: AWS::DataPipeline::Pipeline
  Properties: 
    Name: DynamoDBOutputS3InputHive
    Description: "Pipeline to Import S3 data to DynamoDB"
    Activate: true
    ParameterObjects: 
      - 
        Id: "myDDBWriteThroughputRatio"
        Attributes: 
          - 
            Key: "description"
            StringValue: "DynamoDB read throughput ratio"
          - 
            Key: "type"
            StringValue: "Double"
          - 
            Key: "default"
            StringValue: "0.25"
      - 
        Id: "myInputS3Loc"
        Attributes: 
          - 
            Key: "description"
            StringValue: "S3 output bucket"
          - 
            Key: "type"
            StringValue: "AWS::S3::ObjectKey"
          - 
            Key: "default"
            StringValue: "s3://titan-airflow-data-bucket/TepSimilarItemsRecommender/manual_datapipelineInputTestTxtv2/"
      - 
        Id: "myDDBTableName"
        Attributes: 
          - 
            Key: "description"
            StringValue: "DynamoDB Table Name "
          - 
            Key: "type"
            StringValue: "String"

      -
        Id: "myDDBRegion"
        Attributes: 
          - 
            Key: "description"
            StringValue: "DynamoDB Table Region "
          - 
            Key: "type"
            StringValue: "String"
          -
            Key: "default"
            StringValue: ap-south-1
    ParameterValues: 
      - 
        Id: "myDDBTableName"
        StringValue: SimilarItemsTest
    PipelineObjects: 
      - 
        Id: "S3InputDataNode"
        Name: "Copy data to this S3 location"
        Fields: 
          - 
            Key: "type"
            StringValue: "S3DataNode"
          - 
            Key: "directoryPath"
            StringValue: "#{myInputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}"
      - 
        Id: "DDBDestinationTable"
        Name: "DDBDestinationTable"
        Fields: 
          - 
            Key: "tableName"
            StringValue: "#{myDDBTableName}"
          - 
            Key: "type"
            StringValue: "DynamoDBDataNode"
          # - 
          #   Key: "dataFormat"
          #   RefValue: "DDBExportFormat"
          - 
            Key: "writeThroughputPercent"
            StringValue: "#{myDDBWriteThroughputRatio}"
      # - 
      #   Id: "DDBExportFormat"
      #   Name: "DDBExportFormat"
      #   Fields: 
      #     - 
      #       Key: "type"
      #       StringValue: "DynamoDBExportDataFormat"
      - 
        Id: "TableLoadActivity"
        Name: "TableLoadActivity"
        Fields: 
          - 
            Key: "resizeClusterBeforeRunning"
            StringValue: "false"
          - 
            Key: "type"
            StringValue: "EmrActivity"
          - 
            Key: "input"
            RefValue: "S3InputDataNode"
          - 
            Key: "runsOn"
            RefValue: "EmrClusterForLoad"
          - 
            Key: "output"
            RefValue: "DDBDestinationTable"
          - 
            Key: "maximumRetries"
            StringValue: "2"
          - 
            Key: "step"
            StringValue: "s3://dynamodb-dpl-#{myDDBRegion}/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar,org.apache.hadoop.dynamodb.tools.DynamoDBImport,#{input.directoryPath},#{output.tableName},#{output.writeThroughputPercent}"
      # - 
      #   Id: "DefaultSchedule"
      #   Name: "RunOnce"
      #   Fields: 
      #     - 
      #       Key: "occurrences"
      #       StringValue: "1"
      #     - 
      #       Key: "startAt"
      #       StringValue: "FIRST_ACTIVATION_DATE_TIME"
      #     - 
      #       Key: "type"
      #       StringValue: "Schedule"
      #     - 
      #       Key: "period"
      #       StringValue: "1 Day"
      - 
        Id: "Default"
        Name: "Default"
        Fields: 
          - 
            Key: "type"
            StringValue: "Default"
          - 
            Key: "scheduleType"
            StringValue: "ONDEMAND"
          - 
            Key: "failureAndRerunMode"
            StringValue: "CASCADE"
          - 
            Key: "role"
            StringValue: "DataPipelineDefaultRole"
          - 
            Key: "resourceRole"
            StringValue: "DataPipelineDefaultResourceRole"
          - 
            Key: "pipelineLogUri"
            StringValue: "s3://titan-airflow-data-bucket/TepSimilarItemsRecommender/datapipelinelogs/"
      - 
        Id: "EmrClusterForLoad"
        Name: "EmrClusterForLoad"
        Fields: 
          - 
            Key: "terminateAfter"
            StringValue: "2 Hours"
          - 
            Key: "amiVersion"
            StringValue: "3.3.2"
          - 
            Key: "masterInstanceType"
            StringValue: "t1.micro"
          - 
            Key: "coreInstanceType"
            StringValue: "t1.micro"
          - 
            Key: "coreInstanceCount"
            StringValue: "1"
          - 
            Key: "type"
            StringValue: "EmrCluster"
          - 
            Key: "subnetId"
            StringValue: "subnet-08dfb50f8622bac04"
          - 
            Key: "region"
            StringValue: "ap-south-1"
